---
title: "Methodological advancements on the use of administrative data in Official Statistics - User Manual 2"
output: 
  word_document:
    toc: true  
date: "Dec 2022"
editor_options:
  markdown:
    wrap: 72
---
<!-- output: -->
<!--   html_document: -->
<!--     toc: true -->
<!--     toc_float: true -->
<!--     theme: yeti -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, eval = T, include = FALSE}
        library(xaringan)
        # To livepreview, type in the console xaringan::inf_mr()
        source("2_Prep_Wtsample_Freq_Table.R")
```

# Overview of Quality measures indicators

To measure the quality of administrative data, we need a benchmark data.
It is ideal if a small sample of **Census** data is available. In such
scenario, we have information on the ***true*** population distribution.
However, we may not have access to Census data. In this case, one can
use **weighted sample survey data** as a source of benchmark, which is
used as reference to administrative data.

## What are distance metrics?

Distance metrics is one of the methods that can assess similarity of the
administrative data in reference to weighted survey counts (or census
data).

To calculate distance metrics, we first obtain one-way, and two-way
frequency tables of categorical variables, and calculates proportions of
each sub-category by each data source. Next, we combines master (benchmark) and admin freq tables. Then, we create domains for quality
indicators, and finally compute distance metrics as part of quality
indicators. We offer three different types of distance metrics. To allow comparison across the metrics, we standardise the calculations.

Users can also produce a summary table of three types of distance metrics, and visualise the results.


## What are R-indicators?

EXPLAIN HERE

# RUNNING 2_Prep_Wtsample_Freq_Table.R

This code file consists of two parts: generating a weighted sample data,
and preparing an auxiliary file for R-indicators.

## PREP PART 1: Generate a weighted sample data

-   Open the `2_Prep_Wtsample_Freq_Table.R` file.
-   Step 1: Load a small percentage of Census data. It is called
    **pop_u\_short_before_sim_5vars** in the provided example code.

```{r eval=F, error = F}
        #H---------------------------------------
        ##> 1. Load Census data
        #H--------------------------------------

        load("pop_u_short_before_sim_5vars.RData")
        df  <- pop_u_short_before_sim_5vars
        dim(df)   # obs = 1163659
        names(df)
```

> In our example Census data, we have 1,163,659 observations with five
> categorical variables including `geography`, `sex`, `age groups`,
> `ethnic groups`, and `economic activity status`. One can declare which
> variable to tabulate. Here, we declare all five variables using `var` object^[As the `var` object is treated as global macro, the programme runs automatically using the
information stored in global macro, and produces the results.].

```{r, eval = F, include = TRUE}
  var <- c("geog1", "sex", "agecode1",
            "eth_code5", "econg")
```

The description of categories, and distribution is shown below.

| Variable  | Category | Description                           | N       | (%)    |
|-------------|-------------|----------------------|-------------|-------------|
| Total     |          |                                       | 1163659 |        |
| geog1     | 1        | LA codes                              | 116128  | (10.0) |
|           | 2        | LA codes                              | 150139  | (12.9) |
|           | 3        | LA codes                              | 137520  | (11.8) |
|           | 4        | LA codes                              | 170624  | (14.7) |
|           | 5        | LA codes                              | 90873   | (7.8)  |
|           | 6        | LA codes                              | 498375  | (42.8) |
| sex       | 1        | male                                  | 564905  | (48.5) |
|           | 2        | female                                | 598754  | (51.5) |
| agecode1  | 1        | 16-20                                 | 82426   | (7.1)  |
|           | 2        | 21-25                                 | 94643   | (8.1)  |
|           | 3        | 26-30                                 | 110296  | (9.5)  |
|           | 4        | 31-35                                 | 120398  | (10.3) |
|           | 5        | 36-40                                 | 119393  | (10.3) |
|           | 6        | 14-45                                 | 101711  | (8.7)  |
|           | 7        | 46-50                                 | 94209   | (8.1)  |
|           | 8        | 51-55                                 | 100159  | (8.6)  |
|           | 9        | 56-60                                 | 77799   | (6.7)  |
|           | 10       | 61-65                                 | 65833   | (5.7)  |
|           | 11       | 66-70                                 | 57305   | (4.9)  |
|           | 12       | 71-75                                 | 51263   | (4.4)  |
|           | 13       | 76-80                                 | 43678   | (3.8)  |
|           | 14       | 81+                                   | 44546   | (3.8)  |
| eth_code5 | 1        | White                                 | 1081812 | (93.0) |
|           | 2        | Mixed/Multiple ethnic groups          | 10487   | (0.9)  |
|           | 3        | Asian/Asian British                   | 46446   | (4.0)  |
|           | 3        | Black/African/Caribbean/Black British | 16268   | (1.4)  |
|           | 4        | Other ethnic group                    | 8646    | (0.7)  |
| econg     | 1        | In employment(FT, PT)                 | 689140  | (59.2) |
|           | 2        | Unemployed                            | 27744   | (2.4)  |
|           | 3        | Out of workforce                      | 446775  | (38.4) |

> Using this prior information on the population distribution (based on
> Census), we can mimic the distribution in a random sample. See the
> next step.

-   Step 2: Then, we draw a random sample 1:50.
-   Step 3: From the randomly selected sample (`1163659/50 = 23273`), we
    then obtain frequency table of categorical variables (count of
    categories). Users can run the pre-defined function,
    `fn_maxvar5_freq_table()` to perform the task. The function[^1]
    automatically obtains counts and structure the output in long form,
    organised by each variable, and by its discrete category.

[^1]: We also provide `fn_maxvar4_freq_table()` for users who declare
    four categorical variables

```{r, eval = F, include = T}
        fn_maxvar5_freq_table()
```

> This procedure is to *assess distribution* of categories in a random
> sample (N = 23273). Based on the counts of the randomly selected
> sample, we multiply the counts by 50. One may wonder why we multiply.
> As we *reduced* the census sample by drawing a random sample by the
> 1:50 ratio, we need to *convert* the shrank sample back to the
> original size (with the priori distribution). That's why we multiply
> by 50 (Weighted Sample N = 23273 \* 50 = 1163650). This completes the
> process of generating weighted sample survey data.

-   Step 4: Carry out checks to see if the calculated frequency tables
    are accurate.

```{r }
        freq_table[1:8, 1:9]
```

> When we printed the first 8 lines and 10 variables, we can see the
> count, `n`, and the corresponding proportion, `p` by each variable.
> The following code obtains the total observation size and confirms
> that the total proportion adds up to 1, for `geog1` variable. Here,
> the total observation size can be viewed as the population size.

```{r }
        # Check whether the total adds up to 1
        sum(freq_table[1:6, "n"])
        sum(freq_table[1:6, "p"])

```

-   Step 5: Rename and save.

-   Step 6: Export the output frequency table in Excel with the file
    name, **Weightedsample_freq_table.xlsx**.

-   Step 7: Save the R objects as RData. Done.

## PREP PART 2: Auxiliary file for R-indicators

Thus far, we have created a weighted sample data based on the random
sampling procedure, and obtained both one-way and two-way frequency
tables. We also identified the population size of 1,116,350 (`popsize` =
1163650).

For R-indicator calculations, we need to compute `meanpop` by variables
which are `geography`, `sex`, `age groups`, `ethnic groups`, and
`economic activity status`.

```{r, eval = F, include = TRUE}
        # Compute meanpop
        auxiliary  <- freq_table %>%
                  filter(oneway == 1) %>%
                  group_by(by1) %>%
                  mutate(meanpop = n / popsize) %>%
                  ungroup()  %>%
                  dplyr::select(seq, count = n, by1,
                   v, by2, meanpop, raw_n)
```

To do so, we first remove two-way and keep the one-way frequency table
only. Then, by variable-level, which is indicated by `by1`, we compute
`meanpop`. As seen before, the count of each category is stored in `n`.
The `meanpop` is obtained by dividing `n` by `popsize`. For example, the
value of **meanpop** for first category of `geog1` is calculated as
`113250/1163650 = 0.0973`, and the second category of `geog1` is
`148400/1163650 = 0.1275` and so on.

Finally, we add the `popsize` in the first row, to complete the
Auxiliary file.

Let's look at the Auxiliary file:

```{r}
        print(wtsample_auxiliary_econg)
```

# RUNNING 3_A_Distance_Metrics.R

```{r, eval = T, include = FALSE}
        source("3_A_Distance_Metrics.R")
```

This file allows us to compute distance metrics.

The preliminary step is to ensure we have benchmark data. Simply
*source* the previous file as below to update it.

```{r, eval = F, include = TRUE}
        source("2_Prep_Wtsample_Freq_Table.R")
```

-   Step 1: read admin data

```{r}
        df  <- read_csv("public_release_admin.csv")
        tail(df)
```

The last six observations of the example admin data is shown above. As
the person id goes up to 1033664, we can see there are `1033664`
observations in admin data. We also notice that `geog1a` is used instead
of *geog1*. As such, we declare all five variables for tabulations.

```{r, eval = F, include = T}
  var <- c("geog1a", "sex", "agecode1",
            "eth_code5", "econg")
```

-   Step 2: Obtain admin freq tables. 

As mentioned in the previous section, we obtain frequency table of categorical variables (count of categories). Users can run the pre-defined function,
    `fn_maxvar5_freq_table()` to perform the task. The function[^2]
    automatically obtains counts and structure the output in long form,
    organised by each variable, and by its discrete category.

[^2]: We also provide `fn_maxvar4_freq_table()` for users who declare
    four categorical variables

Once the frequency tables are obtained, we rename the object as
`Admin_f_table_one`. Let's inspect `Admin_f_table_one`.
<!-- table width no problems. All prints fine!!! -->

```{r}
        head(Admin_f_table_one)
```

```{r}
       tail(Admin_f_table_one)
```

-   Step 3: Merge admin + Weighted sample freq tables

```{r, eval = F, include = TRUE}
       fn_merge_one_admin_wtsample_f_table_temp()
```

The code above executes merging two data sources.

-   Step 4: Create domains of quality indicators

```{r, eval = F, include = TRUE}
        fn_create_domain_temp()
```

To check the domains, we can use *Janitor* package's `tabyl`
function[^3]. The function creates 15 domains, including five single
variables' domain, and ten bivariate domains.

[^3]: This is essentially almost identical to
    `table(display_domain$fct_domain)`, but the approach `tabyl`
    produces percent by default

```{r}
        display_domain %>% tabyl(fct_domain)
```

-   Step 5: Compute distance metrics


Run the functions to compute three types of distance metrics. 
```{r, eval = F, include = T}
        fn_unstd_distance_metrics_full()
        fn_unstd_distance_metrics_tidy()
```
The functions draw on the formulae as given below. 
![Distance metrics](User_manual/Distance_metrics_2.png)

-   Step 6: Standardise distance metrics

-   Step 7: Reshape, then tidy


From wide form, the outputs have been reshaped to long form. Then, we keep standardised solutions. The results are as follows:
```{r}
      df <- distance_metrics_long
      # std_test(1-Duncan, 1-HD, 1-KL) only
      df <- df %>% filter(std_test_use == 1)

      df[1:9, c(1:2, 4:5, 9)]
```

-   Step 8: Plot the distance metrics

```{r}
         plot(p)
```

# RUNNING 3_B_R-indicator.R

```{r, eval = T, include = FALSE}
        rm(list = ls())
        source("3_B_R-indicator.R")
```

The file computes the overall R-indicator. Users
can also proceed with computing **partial** R-indicators by category level, and variable level. The procedure can be computationally extensive. This is noted in the relevant section, so that users can allow some time to execute the code.

-   Step 1: Benchmark mean population ready

Load auxiliary file and remove the last category
From each categorical variable (`group_by(by1)`), we identify the last category (` max(by2)`). We then remove the last category(`dplyr::select(-c(lastcat_, lastcat))`). Here, we explicitly instruct R to use `dplyr` package to access `select` function^[This is to avoid warnings messages from R when R searches for a particular function from two different packages.].
```{r, eval = F, echo = F, include = T}
        col_auxiliary <- auxiliary_temp %>%
             group_by(by1) %>%
             mutate(
              lastcat_ = ifelse(by2 == max(by2),
                    1, 0),
              lastcat = ifelse(seq == 1, 0, lastcat_)
            ) %>%
             filter(lastcat == 0) %>%
             dplyr::select(-c(lastcat_, lastcat)) %>%
             ungroup()
```
We only need `meanpop`. It is shown in a single column (column vector). Let's check.
```{r, eval = T, include = T}
        # Print meanpop
        print(col_auxiliary[1:nrow(col_auxiliary), "meanpop"],
            n = nrow(col_auxiliary))
```

-   Step 2: Keep wtsample distributions as row vectors


We then transpose `meanpop` to a single row format (row vector).
Now, our benchmark data is ready. The next step is open the corresponding administrative data, and carry out computing R-indicators.

-   Step 3: Compute R-indicators


Users can use pre-defined functions. Please allow a minute to execute.
```{r, eval = F, include = T}
        aa  <- read_csv("public_release_admin.csv")

        nrow(aa) # 1033664

        df      <- NULL
        between <- NULL
        partial <- NULL
        partialtemp <-  NULL
        fn_overall_r_indicator_1()
        fn_overall_r_indicator_2()
        fn_overall_r_indicator_3()
        fn_overall_r_indicator_4()
```
Users can consult `Functions/2_Functions_R-indicators.R` file for more details and operationalisation.

-   Step 4: Save in Excel and inspect


At this stage, users can inspect accordingly. Let's have a look. Here, we can see the overall R-indicator is estimated as 0.496 based on the administrative data (N=1033664). Looking at the variable-level R-indicator (see rows 4-8), geog1a was seen to have the greatest R-indicator (0.04) compared to econg (0.0002).
```{r}
    partial[1:17, c(1:2, 4, 8:10)] 
```

-   Step 5: Scatterplot


R-indicator by the variable level.
```{r, eval = T, include = T}
      plot(p1)
```

R-indicator by the category-level.
```{r, eval = T, include = T}
       plot(p2)
```

This concludes the manual. Thank you for taking the time reading the material. Please get in touch with any query or errata at <fanfurcada@gmail.com>.





